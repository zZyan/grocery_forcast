{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = 'whitegrid', color_codes = True)\n",
    "%matplotlib inline\n",
    "\n",
    "#For statistical tests\n",
    "import scipy.stats as st\n",
    "\n",
    "#For formula notation (similar to R)\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CAUTION!! : load full data\n",
    "# # load complete train data\n",
    "# types_dict = {'id': 'int32',\n",
    "#              'item_nbr': 'int32',\n",
    "#              'store_nbr': 'int8',\n",
    "#              'unit_sales': 'float32',\n",
    "#              'onpromotion': 'bool'}\n",
    "\n",
    "# train_df_full = pd.read_csv('data/train.csv',low_memory=True,  parse_dates=['date'],dtype=types_dict)\n",
    "\n",
    "# ## save in feather for easy reload \n",
    "# train_df_full.to_feather('data/train.feat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_full = pd.read_feather('data/train.feat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Time Info\n",
    "1. check the time distribution\n",
    "2. select subset of time to speed up processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the time feature\n",
    "train_df_full['year'] = train_df_full['date'].dt.year\n",
    "train_df_full['month'] = train_df_full['date'].dt.month\n",
    "train_df_full['day'] = train_df_full['date'].dt.day\n",
    "train_df_full['quater'] = train_df_full['date'].dt.quarter\n",
    "# check the distribution\n",
    "print(np.unique(train_df_full.year))\n",
    "print(np.unique(train_df_full.month))\n",
    "print(np.unique(train_df_full.day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if fully loaded\n",
    "# select August Data and date > 15 as subdataset\n",
    "train_df1 = train_df_full[train_df_full.year == 2017]\n",
    "train_df2 = train_df_full[train_df_full.year == 2016];\n",
    "train_df2 = train_df2[train_df2.month >=8]\n",
    "train_df = pd.concat([train_df1,train_df2])\n",
    "display(train_df.head(2))\n",
    "# clean unuseful raw data\n",
    "del train_df_full;del train_df1; del train_df2; gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load item, store, oil and transaction info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load store info\n",
    "store_df = pd.read_csv('data/stores.csv',dtype = {'type':'category','cluster':'category','city':'category','state':'category'})\n",
    "# load item info\n",
    "items_df = pd.read_csv('data/items.csv',dtype = {'family':'category','perishable':bool,'class':'category'})\n",
    "# load oil info\n",
    "oil_df = pd.read_csv('data/oil.csv')\n",
    "#load transacrion\n",
    "trans_df = pd.read_csv(\"data/transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\",len(items_df['family'].unique()),\"families of products or items\")\n",
    "print(\"There are\",len(store_df['type'].unique()),\"type of stores\")\n",
    "print(\"Stores are in \",len(store_df['city'].unique()),\"cities in \", len(store_df['state'].unique()),\"states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load holiday information \n",
    "1. assign the national holoday to all the items at cetain date\n",
    "2. assign the local holiday to item at certain city at certain date\n",
    "3. no correspnondling regional holiday info, ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load holiday info\n",
    "holiday_df = pd.read_csv('data/holidays_events.csv',parse_dates=['date'],dtype= {'transferred':np.bool})\n",
    "holiday_df = holiday_df.query('transferred == False')\n",
    "holiday_df=holiday_df.drop(['transferred','description'],axis = 1)\n",
    "# subdivide holiday information\n",
    "national_holiday_df = holiday_df[holiday_df.locale == 'National']\n",
    "national_holiday_df.drop(['locale'],axis = 1)\n",
    "regional_holiday_df = holiday_df[holiday_df.locale == 'Regional']\n",
    "regional_holiday_df.drop(['locale'],axis = 1)\n",
    "local_holiday_df = holiday_df[holiday_df.locale == 'Local']\n",
    "local_holiday_df.drop(['locale'],axis = 1)\n",
    "# check the holiday distribution\n",
    "# only need to merge city for local holiday\n",
    "unique_locale = np.unique(local_holiday_df.locale_name)\n",
    "unique_city = np.unique(store_df.city)\n",
    "unique_state = np.unique(store_df.state)\n",
    "\n",
    "print ([k if k in unique_city else '' for k in unique_locale])\n",
    "print ([k if k in unique_state else '' for k in unique_locale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Feature Reduction\n",
    "\n",
    "refer to https://www.kaggle.com/sohinibhattacharya86/predict-grocery-sales-rf-xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 - Is there any statistically significant relation between Store Type and Cluster of the stores ?\n",
    "\n",
    "1. Null Hypothesis H0 = Store Type (a, b, c, d, e) and Cluster (1 to 17) are independent from each other.\n",
    "2. Alternative Hypothesis HA = Store Tpe and cluster are not independent of each other. There is a relationship between them.\n",
    "3. Store Type - categorical variable\n",
    "4. Cluster - categorical variable\n",
    "\n",
    "Now, to determine if there is a statistically significant correlation between the variables, we use a chi-square test of independence of variables in a contingency table\n",
    "\n",
    "Here, we create a contingency table, with the frequencies of all possible values\n",
    "\n",
    "** Conclusion ** cluster is depend on store type, so drop store type to reduce the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency table\n",
    "ct = pd.crosstab(store_df['type'], store_df['cluster'])\n",
    "display(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 - Is there any statistically significant relation between Promotion and unit scales ?\n",
    "\n",
    "1. Null Hypothesis H0 = Promotion and Sales are independent from each other.\n",
    "2. Alternative Hypothesis HA = Promotion and Sales are not independent of each other. There is a relationship between them.\n",
    "3. Promotion - categorical variable - Independent variable\n",
    "4. Sales - continuous variable - Dependent variable\n",
    "\n",
    "Now, to determine if there is a statistically significant correlation between the variables, we use a student t test\n",
    "\n",
    "2-sample t-test: testing for difference across populations\n",
    "\n",
    "** Conclusion ** There is a relationship between onpromotion and sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_sales = train_df[train_df['onpromotion'] == 1.0]['unit_sales']\n",
    "nopromo_sales = train_df[train_df['onpromotion'] == 0.0]['unit_sales']\n",
    "st.ttest_ind(promo_sales, nopromo_sales, equal_var = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 - Is there any statistically significant relation between Oil price and Sales of the stores ?\n",
    "\n",
    "1. Null Hypothesis H0 = Oil price and Sales are independent from each other.\n",
    "2. Alternative Hypothesis HA = Oil price and Sales are not independent of each other. There is a relationship between them.\n",
    "3. Oil Price - Independent continuous variable\n",
    "4. Sales - Dependent continuous variable\n",
    "\n",
    "conlusion: ** ??? ** (Drop first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining multiple features\n",
    "- drop: \n",
    "1. oil.csv, because we are only dealing with 2017, so assume oil price does not have so quick influence\n",
    "2. cluster in store (efficiency reason)\n",
    "- disregard for now: transections.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### version1: nearly all joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the store information\n",
    "train = pd.merge(train_df, store_df, on= \"store_nbr\",how = 'left')\n",
    "# join the item feature\n",
    "train = pd.merge(train, items_df, on= \"item_nbr\",how = 'left')\n",
    "# join holiday info\n",
    "train = pd.merge(train,national_holiday_df[['date', 'type']],on = 'date', how='left', suffixes=['', '_nationalHoliday'])\n",
    "train = pd.merge(train, local_holiday_df[['date','type','locale_name']], \n",
    "                 left_on = ['date','city'], right_on=['date','locale_name'], \n",
    "                 how='left', suffixes=['', '_cityHoliday'])\n",
    "train = train.drop(['locale_name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train.unit_sales<0),'unit_sales'] = 0 \n",
    "train['unit_sales_cal'] =  train['unit_sales'].apply(pd.np.log1p) \n",
    "\n",
    "train['type_nationalHoliday'] = train['type_nationalHoliday'].fillna('no')\n",
    "train['type_cityHoliday'] = train['type_cityHoliday'].fillna('no')\n",
    "train['type_nationalHoliday'] = train['type_nationalHoliday'].replace({'Additional': 'Holiday','Transfer':'Holiday','Additional':'Holiday'})\n",
    "train['type_cityHoliday'] = train['type_cityHoliday'].replace({'Additional': 'Holiday','Transfer':'Holiday','Additional':'Holiday'})\n",
    "\n",
    "train['type_nationalHoliday'] = train['type_nationalHoliday'].astype('category')\n",
    "train['type_cityHoliday'] = train['type_cityHoliday'].astype('category')\n",
    "train['onpromotion'] = train['onpromotion'].astype('category')\n",
    "train['city'] = train['city'].astype('category')\n",
    "\n",
    "cat_columns = train.select_dtypes(['category']).columns\n",
    "train[cat_columns] = train[cat_columns].apply(lambda x: x.cat.codes)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing (see training section below)\n",
    "\n",
    "1. according to the training below we choose\n",
    "    - Gradient Boosting: max_depth = 12\n",
    "    - Random Forest: max_depth = 12\n",
    "    - AdaBoost + Decisin Tree: max_depth = 12, n_estimator = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y, pred, w):\n",
    "    from sklearn import metrics\n",
    "    return metrics.mean_squared_error(y, pred, sample_weight=w)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define the training feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "import operator\n",
    "\n",
    "X_fea = [c for c in list(train.columns.values) if c not in ['id','unit_sales','unit_sales_cal','class','date']]\n",
    "Y_fea = ['unit_sales','unit_sales_cal']\n",
    "display(X_fea)\n",
    "display(Y_fea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train all the data using settled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "def rmspe_xg(pred, y):\n",
    "    y = y.get_label()\n",
    "    return \"rmspe\", np.sqrt(np.mean((pred-y) ** 2))\n",
    "gb_train = {'mean':[],'var':[]}; gb_val = {'mean':[],'var':[]}; gb_time = []\n",
    "X_Y_train, X_Y_val = train_test_split(train, test_size=0.2, random_state=0)\n",
    "dtrain = xgb.DMatrix(X_Y_train[X_fea], X_Y_train[Y_fea].unit_sales_cal)\n",
    "dvalid = xgb.DMatrix(X_Y_val[X_fea], X_Y_val[Y_fea].unit_sales_cal)\n",
    "params = {\"objective\": \"reg:linear\",\"booster\" : \"gbtree\",\"eta\": 0.3,\"max_depth\": 13,\"subsample\": 0.9,\"colsample_bytree\": 0.7,\n",
    "          \"silent\": 0,\"seed\": 0}\n",
    "num_boost_round = 30\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "  early_stopping_rounds=10, feval=rmspe_xg, verbose_eval=True)\n",
    "del train;\n",
    "del watchlist; del dtrain;del dvalid;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the validation set (because ground trutch of testing set is not avalilabel on kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Y_val2=X_Y_val.head(100)\n",
    "pred =  gbm.predict(xgb.DMatrix(X_Y_val2[X_fea]))\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "ax1 = plt.subplot(231)\n",
    "ax1.plot(X_Y_val2[X_fea].day,pred,'ro',label = 'pred')\n",
    "ax1.plot(X_Y_val2[X_fea].day.head(1000),X_Y_val2[Y_fea].unit_sales_cal,'g^',label = 'ground_truth')\n",
    "ax1.set_xlabel('day')\n",
    "ax1.set_ylabel('logp1(unit_sales)')\n",
    "ax1.legend()\n",
    "ax1.set_title('day vs logp1(unit_sales)')\n",
    "\n",
    "ax2 = plt.subplot(232)\n",
    "ax2.plot(X_Y_val2[X_fea].store_nbr,pred,'ro',label = 'pred')\n",
    "ax2.plot(X_Y_val2[X_fea].store_nbr,X_Y_val2[Y_fea].unit_sales_cal, 'g^',label = 'ground_truth')\n",
    "ax2.legend()\n",
    "ax2.set_xlabel('store_nbr')\n",
    "ax2.set_ylabel('logp1(unit_sales)')\n",
    "ax2.set_title('store_nbr vs logp1(unit_sales)')\n",
    "\n",
    "\n",
    "ax3 = plt.subplot(233)\n",
    "ax3.plot(X_Y_val2[X_fea].family,pred,'ro',label = 'pred')\n",
    "ax3.plot( X_Y_val2[X_fea].family,X_Y_val2[Y_fea].unit_sales_cal,'g^',label = 'ground_truth')\n",
    "ax3.legend()\n",
    "ax3.set_xlabel('family')\n",
    "ax3.set_ylabel('logp1(unit_sales)')\n",
    "ax3.set_title('family vs logp1(unit_sales)')\n",
    "\n",
    "\n",
    "ax4 = plt.subplot(234)\n",
    "ax4.plot(X_Y_val2[X_fea].item_nbr,pred,'ro',label = 'pred')\n",
    "ax4.plot(X_Y_val2[X_fea].item_nbr,X_Y_val2[Y_fea].unit_sales_cal,'g^',label = 'ground_truth')\n",
    "ax4.legend()\n",
    "ax4.set_xlabel('item_nbr')\n",
    "ax4.set_ylabel('logp1(unit_sales)')\n",
    "ax4.set_title('item number vs logp1(unit_sales)')\n",
    "\n",
    "ax5 = plt.subplot(235)\n",
    "ax5.plot(X_Y_val2[X_fea].month,pred,'ro',label = 'pred')\n",
    "ax5.plot(X_Y_val2[X_fea].month,X_Y_val2[Y_fea].unit_sales_cal,'g^',label = 'ground_truth')\n",
    "ax5.legend()\n",
    "ax5.set_xlabel('month')\n",
    "ax5.set_ylabel('logp1(unit_sales)')\n",
    "ax5.set_title('month vs logp1(unit_sales)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(random_state=0,max_depth=12,n_jobs=-1)\n",
    "regr.fit(train[X_fea], train[Y_fea].unit_sales_cal.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada_train = {'mean':[],'var':[]}; ada_val = {'mean':[],'var':[]}\n",
    "X_fea2 = [c for c in X_fea if c not in ['type_cityHoliday','perishable','state','quater','onpromotion']]\n",
    "print('features {}',X_fea2)\n",
    "\n",
    "regr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=12),n_estimators=6, random_state=0)\n",
    "regr.fit(train[X_fea2], train[Y_fea].unit_sales_cal.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## CAUTION!! : load full data\n",
    "# load complete train data\n",
    "types_dict = {'id': 'int32',\n",
    "             'item_nbr': 'int32',\n",
    "             'store_nbr': 'int8',\n",
    "             'unit_sales': 'float32',\n",
    "             'onpromotion': 'bool'}\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv',low_memory=True,  parse_dates=['date'],dtype=types_dict)\n",
    "# join the store information\n",
    "test = pd.merge(test_df, store_df, on= \"store_nbr\",how = 'left')\n",
    "# join the item feature\n",
    "test = pd.merge(test, items_df, on= \"item_nbr\",how = 'left')\n",
    "# join holiday info\n",
    "test = pd.merge(test,national_holiday_df[['date', 'type']],on = 'date', how='left', suffixes=['', '_nationalHoliday'])\n",
    "test = pd.merge(test, local_holiday_df[['date','type','locale_name']], \n",
    "                 left_on = ['date','city'], right_on=['date','locale_name'], \n",
    "                 how='left', suffixes=['', '_cityHoliday'])\n",
    "test = test.drop(['locale_name'], axis=1)\n",
    "\n",
    "test['type_nationalHoliday'] = test['type_nationalHoliday'].fillna('no')\n",
    "test['type_cityHoliday'] = test['type_cityHoliday'].fillna('no')\n",
    "test['type_nationalHoliday'] = test['type_nationalHoliday'].replace({'Additional': 'Holiday','Transfer':'Holiday','Additional':'Holiday'})\n",
    "test['type_cityHoliday'] = test['type_cityHoliday'].replace({'Additional': 'Holiday','Transfer':'Holiday','Additional':'Holiday'})\n",
    "\n",
    "test['type_nationalHoliday'] = test['type_nationalHoliday'].astype('category')\n",
    "test['type_cityHoliday'] = test['type_cityHoliday'].astype('category')\n",
    "test['onpromotion'] = test['onpromotion'].astype('category')\n",
    "test['city'] = test['city'].astype('category')\n",
    "\n",
    "cat_columns = test.select_dtypes(['category']).columns\n",
    "test[cat_columns] = test[cat_columns].apply(lambda x: x.cat.codes)\n",
    "del store_df; del items_df;\n",
    "del national_holiday_df; del local_holiday_df;\n",
    "test['year'] = test['date'].dt.year\n",
    "test['month'] = test['date'].dt.month\n",
    "test['day'] = test['date'].dt.day\n",
    "test['quater'] = test['date'].dt.quarter\n",
    "test.head(1)\n",
    "\n",
    "test['unit_sales'] = gbm.predict(xgb.DMatrix(test[X_fea]))\n",
    "# test['unit_sales'] = regr.predict(test[X_fea])\n",
    "# test['unit_sales'] = regr.predict(test[X_fea2])\n",
    "test['unit_sales'] = np.expm1(test['unit_sales'])\n",
    "test[['id','unit_sales']].to_csv('gb_pred', index=False, float_format='%.5f')\n",
    "# test[['id','unit_sales']].to_csv('ada_pred', index=False, float_format='%.5f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Reference: https://www.kaggle.com/lscoelho/stacking-model-lm-etr-rf-and-gbr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_train = train; del train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV on Gradient Boostting\n",
    "Here I use xgboost instead of the gradietn boosting in sklearn because they are basically the same method but sklearn's implemnet is much slower\n",
    "\n",
    "refer to https://www.kaggle.com/sohinibhattacharya86/predict-grocery-sales-rf-xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import time\n",
    "def rmspe(y,pred):\n",
    "    return np.sqrt(np.mean((pred-y) ** 2))\n",
    "def rmspe_xg(pred, y):\n",
    "    y = y.get_label()\n",
    "    return \"rmspe\", np.sqrt(np.mean((pred-y) ** 2))\n",
    "gb_train = {'mean':[],'var':[]}; gb_val = {'mean':[],'var':[]}; gb_time = []\n",
    "for max_depth in range(8,12): \n",
    "    tmp_train = [];tmp_val = [];tmp_time = []\n",
    "    for k in range(0,3):\n",
    "        X_Y_train, X_Y_val = train_test_split(cur_train, test_size=0.2, random_state=int(11*k))\n",
    "        \n",
    "        dtrain = xgb.DMatrix(X_Y_train[X_fea], X_Y_train[Y_fea].unit_sales_cal)\n",
    "        dvalid = xgb.DMatrix(X_Y_val[X_fea], X_Y_val[Y_fea].unit_sales_cal)\n",
    "        params = {\"objective\": \"reg:linear\",\"booster\" : \"gbtree\",\"eta\": 0.3,\"max_depth\": max_depth,\"subsample\": 0.9,\"colsample_bytree\": 0.7,\n",
    "                  \"silent\": 0,\"seed\": 0}\n",
    "        num_boost_round = 30\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n",
    "          early_stopping_rounds=10, feval=rmspe_xg, verbose_eval=True)\n",
    "        pred = gbm.predict(xgb.DMatrix(X_Y_train[X_fea]))\n",
    "        train_error = cost_function(X_Y_train[Y_fea].unit_sales_cal, pred, X_Y_train.perishable.map({False: 1.0, True: 1.25}))\n",
    "        print('GB - Performance: NWRMSLE(1) = ',train_error)\n",
    "        pred = gbm.predict(xgb.DMatrix(X_Y_val[X_fea]))\n",
    "        val_error = cost_function(X_Y_val[Y_fea].unit_sales_cal,pred,X_Y_val.perishable.map({False: 1.0, True: 1.25}))\n",
    "        print('GB - Performance: NWRMSLE(1) = ',val_error)\n",
    "        del X_Y_train, X_Y_val\n",
    "        tmp_train.append(train_error)\n",
    "        tmp_val.append(val_error)\n",
    "    gb_train['mean'].append(np.mean(tmp_train)); gb_train['var'].append(np.var(tmp_train))\n",
    "    gb_val['mean'].append(np.mean(tmp_val)); gb_val['var'].append(np.var(tmp_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# rf_train = {'mean':[],'var':[]}; rf_val = {'mean':[],'var':[]}; gb_time = []\n",
    "for max_depth in range(12,13): \n",
    "    tmp_train = [];tmp_val = [];tmp_time = []\n",
    "    for k in range(0,3):\n",
    "        X_Y_train, X_Y_val = train_test_split(cur_train, test_size=0.2, random_state=int(11*k))\n",
    "        \n",
    "        regr = RandomForestRegressor(random_state=0,max_depth=max_depth,n_jobs=-1)\n",
    "        regr.fit(X_Y_train[X_fea], X_Y_train[Y_fea].unit_sales_cal.values)\n",
    "        train_error = cost_function(X_Y_train[Y_fea].unit_sales_cal, regr.predict(X_Y_train[X_fea]), X_Y_train.perishable.map({False: 1.0, True: 1.25}))\n",
    "        print('RF - Performance: NWRMSLE(1) = ',train_error)\n",
    "        val_error = cost_function(X_Y_val[Y_fea].unit_sales_cal,regr.predict(X_Y_val[X_fea]),X_Y_val.perishable.map({False: 1.0, True: 1.25}))\n",
    "        print('RF - Performance: NWRMSLE(1) = ',val_error)\n",
    "        del X_Y_train, X_Y_val\n",
    "        tmp_train.append(train_error)\n",
    "        tmp_val.append(val_error)\n",
    "    rf_train['mean'].append(np.mean(tmp_train)); rf_train['var'].append(np.var(tmp_train))\n",
    "    rf_val['mean'].append(np.mean(tmp_val)); rf_val['var'].append(np.var(tmp_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_train)\n",
    "print(rf_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CV on AdaBoost\n",
    "\n",
    "1. use the feature importance analysis above to simply the feature, in order to relieve the efficiency burden\n",
    "    - threshold: relative importance < 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_train = {'mean':[],'var':[]}; dt_val = {'mean':[],'var':[]}\n",
    "X_fea2 = [c for c in X_fea if c not in ['type_cityHoliday','perishable','state','quater','onpromotion']]\n",
    "print('features {}',X_fea2)\n",
    "\n",
    "for max_depth in range(9,13): \n",
    "    tmp_train = [];tmp_val = [];tmp_time = []\n",
    "    for k in range(0,3):\n",
    "        X_Y_train, X_Y_val = train_test_split(cur_train, test_size=0.2, random_state=int(11*k))\n",
    "        regr = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        regr.fit(X_Y_train[X_fea2], X_Y_train[Y_fea].unit_sales_cal.values)\n",
    "        train_error = cost_function(X_Y_train[Y_fea].unit_sales_cal, regr.predict(X_Y_train[X_fea2]), X_Y_train.perishable.map({False: 1.0, True: 1.25}))\n",
    "        print('Adaboost+Decision Tree - Performance: NWRMSLE(1) = ',train_error)\n",
    "        val_error = cost_function(X_Y_val[Y_fea].unit_sales_cal,regr.predict(X_Y_val[X_fea2]),X_Y_val.perishable.map({False: 1.0, True: 1.25}))\n",
    "        print('Adaboost+Decision Tree - Performance: NWRMSLE(1) = ',val_error)\n",
    "        del X_Y_train, X_Y_val\n",
    "        tmp_train.append(train_error)\n",
    "        tmp_val.append(val_error)\n",
    "    dt_train['mean'].append(np.mean(tmp_train)); dt_train['var'].append(np.var(tmp_train))\n",
    "    dt_val['mean'].append(np.mean(tmp_val)); dt_val['var'].append(np.var(tmp_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada_train = {'mean':[],'var':[]}; ada_val = {'mean':[],'var':[]}\n",
    "X_fea2 = [c for c in X_fea if c not in ['type_cityHoliday','perishable','state','quater','onpromotion']]\n",
    "print('features {}',X_fea2)\n",
    "\n",
    "for n_estimators in range(2,16,2): \n",
    "    tmp_train = [];tmp_val = [];tmp_time = []\n",
    "    for k in range(0,3):\n",
    "        X_Y_train, X_Y_val = train_test_split(cur_train, test_size=0.2, random_state=int(11*k))\n",
    "        regr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=12),n_estimators=n_estimators, random_state=0)\n",
    "        regr.fit(X_Y_train[X_fea2], X_Y_train[Y_fea].unit_sales_cal.values)\n",
    "        train_error = cost_function(X_Y_train[Y_fea].unit_sales_cal, regr.predict(X_Y_train[X_fea2]), X_Y_train.perishable.map({False: 1.0, True: 1.25}))\n",
    "        print('Adaboost+Decision Tree - Performance: NWRMSLE(1) = ',train_error)\n",
    "        val_error = cost_function(X_Y_val[Y_fea].unit_sales_cal,regr.predict(X_Y_val[X_fea2]),X_Y_val.perishable.map({False: 1.0, True: 1.25}))\n",
    "        print('Adaboost+Decision Tree - Performance: NWRMSLE(1) = ',val_error)\n",
    "        del X_Y_train, X_Y_val\n",
    "        tmp_train.append(train_error)\n",
    "        tmp_val.append(val_error)\n",
    "    ada_train['mean'].append(np.mean(tmp_train)); ada_train['var'].append(np.var(tmp_train))\n",
    "    ada_val['mean'].append(np.mean(tmp_val)); ada_val['var'].append(np.var(tmp_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ada_train)\n",
    "print(ada_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "\n",
    "def plotFeaforDict(feature_importance_dict):\n",
    "    # make importances relative to max importance\n",
    "    feature_importance = list(feature_importance_dict.values())\n",
    "    names = list(feature_importance_dict.keys())\n",
    "    feature_importance = 100.0 * (feature_importance / np.max(feature_importance))\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, [names[i] for i in sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "    \n",
    "plotFeaforDict(gbm.get_fscore())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
